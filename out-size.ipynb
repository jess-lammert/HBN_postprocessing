{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd2e717-d56c-4a5e-8a18-4d155d3f78c3",
   "metadata": {},
   "source": [
    "Description: This script checks the size of the .out files in the output>code>jobs directory to determine whether fmriprep was completed for a given participant. File size <10kb fmriprep did not start, >400kb likely complete, ~200-300kb partially complete/error. Incomplete participants may be diagnosed and processed again at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975ba74f-cdc9-4fed-afe3-668963e6c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469eca72-13d5-41e1-b41b-d52367750861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to run .py from command line\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='Sorts a BIDS data directory and give a summary of T1w, fieldmap, and fMRI data.')\n",
    "\n",
    "#parser.add_argument('--bids_dir', help='Path to the bids directory', type=str)\n",
    "#parser.add_argument('--out_dir', help='Out put path where you want the summary tables to be saved', type=str)\n",
    "                    \n",
    "#args = parser.parse_args()\n",
    "#bids_dir = args.bids_dir\n",
    "#out_dir = args.out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c205afa-b841-49cd-989a-fb082b562d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define directories for testing\n",
    "\n",
    "jobs_dir = '/cifs/butler/HBN_data/TD_test_set_output/code/jobs' #contains .out files\n",
    "pp_out = '/cifs/butler/HBN_data/preprocessing/postprocessing_outputs' #save .csv here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cd241d-ce19-4930-9de7-11679df39cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "def list_files(path, *keywords): #keyword is type of file (e.g., \"*T1w.nii.gz*\", \"*bold.nii.gz*\", \"*fMRI_epi.nii.gz*\")\n",
    "    os.chdir(path)\n",
    "    files = []\n",
    "    for keyword in keywords:\n",
    "        files.extend(glob.glob(keyword))\n",
    "    return [os.path.basename(file) for file in files] #returns list of files in given directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa287c5-46a5-4232-b661-b182aea89019",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = list_files(jobs_dir, \"*.out*\") #get .out filenames\n",
    "\n",
    "out_content = []\n",
    "for file in out_list:\n",
    "    with open(jobs_dir + \"/\" + file) as f:\n",
    "        data = f.read()#read .out file contents\n",
    "    out_content.append(data)\n",
    "\n",
    "out_ids = [] #get participant ID from each file\n",
    "for o in out_content:\n",
    "    p_id = o.partition(\"participant_label \")[2] #find ID after \"participant_label \"\n",
    "    p_id = p_id[0:12] #remove everything after the ID (first 12 characters)\n",
    "    out_ids.append(p_id)\n",
    "    \n",
    "out_size = [] #get size of each .out file\n",
    "for file in out_list:\n",
    "    size_b = os.stat(jobs_dir + \"/\" + file)\n",
    "    size_kb = size_b.st_size / 1000 #convert size to kb\n",
    "    out_size.append(size_kb)\n",
    "    \n",
    "#combine lists into df\n",
    "size_df = pd.DataFrame({'file_name': out_list, 'p_id': out_ids, 'size_kb': out_size})\n",
    "\n",
    "#clean df\n",
    "mask = size_df[\"p_id\"].str.startswith(\"NDA\") #remove non-IDs (does not start with NDA)\n",
    "clean_df = size_df[mask]\n",
    "max_size = clean_df.groupby('p_id').max().reset_index() #gives file with the largest size for each unique participant id\n",
    "max_df = pd.DataFrame({'p_id': max_size[\"p_id\"], 'size_kb': max_size[\"size_kb\"], 'file_name': max_size[\"file_name\"]}) #organize in df\n",
    "\n",
    "#add corresponding categories based on description above\n",
    "#if max_df[\"size_kb\"] < 10:\n",
    "#    max_df[\"status\"] = \"not started\"\n",
    "\n",
    "max_df['status'] = np.where(max_df.size_kb < 10,'not started', 'unknown')\n",
    "max_df['status'] = np.where(max_df.size_kb > 400,'likely complete', max_df['status'])\n",
    "max_df['status'] = np.where((max_df.size_kb > 200) & (max_df.size_kb < 400),'partial/error', max_df['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8697f8e5-4740-4493-9c51-886999bc04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df.to_csv(pp_out+'/out-size_all.csv', sep=',', index=False)\n",
    "\n",
    "#get df of incomplete participants\n",
    "df_incomp = max_df[(max_df[\"status\"] == \"not started\") | (max_df[\"status\"] == \"unknown\") | (max_df[\"status\"] == \"partial/error\")]\n",
    "df_incomp.to_csv(pp_out+'/out-size_incomp.csv', sep=',', index=False)\n",
    "\n",
    "#likely complete participants\n",
    "df_comp = max_df[max_df[\"status\"] == \"likely complete\"]\n",
    "df_comp.to_csv(pp_out+'/out-size_comp.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
